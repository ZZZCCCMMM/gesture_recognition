{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6448dfc-1486-4ad2-b838-dbf5f7dabbb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import mediapipe as mp\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) #color conversion\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "#Open cam and release\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Setting Mediapipe\n",
    "mp_holistic = mp.solutions.holistic  # holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils  # drawing utils\n",
    "\n",
    "# Access Mediapipe Model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        #Make decttions\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        #print(results)\n",
    "        \n",
    "        #Draw Landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        #Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) &0xFF == ord('q'):\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0edd5ca9-9b6d-4c44-b50c-a68fa8f69fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    #Draw the face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1) \n",
    "                             )\n",
    "    #Draw the pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=4) \n",
    "                             )\n",
    "    #Draw the left_hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2) \n",
    "                             )\n",
    "    #Draw the right_hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddce0fc4-2c79-4e96-9e4d-0f6d34896de6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'draw_landmarks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdraw_landmarks\u001b[49m(frame,results)\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'draw_landmarks' is not defined"
     ]
    }
   ],
   "source": [
    "#draw_landmarks(frame,results)\n",
    "#plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f815b59-6119-4e55-ac50-d36a175462a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the landmarks as Numpy\n",
    "def extract_keypoints(results):\n",
    "    pose = np.array(list([res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark)).flatten() if results.pose_landmarks else np.zeros(132)\n",
    "    lh = np.array(list([res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark)).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array(list([res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark)).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    face = np.array(list([res.x, res.y, res.z] for res in results.face_landmarks.landmark)).flatten() if results.face_landmarks else np.zeros(1404)\n",
    "    return np.concatenate([pose, face, lh, rh])\n",
    "extract_keypoints(results).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a2fbaf-e88b-4f50-932b-3da04b42828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_Date')\n",
    "\n",
    "#Actions that we try to detect\n",
    "actions = np.array(['hello', 'thank', 'iloveu'])\n",
    "\n",
    "#Thirty video worth of data\n",
    "no_sequences = 30\n",
    "\n",
    "#Video are going to be 30 frames in length\n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1d7e33-756b-41c8-8dad-9ca3501138a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create folder squence\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4c6c25-1776-4823-8c4b-f552f3826e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect action data\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Access Mediapipe Model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    for action in actions:\n",
    "        #Loop through sequences aka videos\n",
    "        for sequence in range(no_sequences):\n",
    "            #Loop through video length aka sequence length\n",
    "            for frame_num in range(sequence_length):\n",
    "        \n",
    "        \n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                #Make decttions\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "                #print(results)\n",
    "\n",
    "                #Draw Landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "                \n",
    "                #NEW Apply wait logic\n",
    "                if frame_num == 0:\n",
    "                    cv2.putText(image, 'START COLLECTION', (120,200),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 4, cv2.LINE.AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 1, cv2.LINE.AA)\n",
    "                    cv2.waitKey(2000)\n",
    "                else:\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 1, cv2.LINE.AA)\n",
    "                \n",
    "                #NEW Export keypoints\n",
    "                keypoints = extract_keypoint(result)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_PATH, keypoints)\n",
    "                \n",
    "                #Show to screen\n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "                if cv2.waitKey(10) &0xFF == ord('q'):\n",
    "                    break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bd2fc87-1780-40cc-83c4-bd9abd9fea9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save the landmark dataset each frame in a single folder\n",
    "result_test = extract_keypoints(results)\n",
    "np.save('0', result_test)\n",
    "np.load('0.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "327472a8-c3c0-41c7-b2ee-3bf467fa127e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "287cb250-c80c-4665-94af-66739e0d045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "face = np.array(list([res.x, res.y, res.z] for res in results.face_landmarks.landmark)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa6f8e98-500e-4c05-9f8e-36f75c7b2922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.46034628,  1.0644002 , -0.00614396, ...,  0.60176933,\n",
       "        0.88338017, -0.01222336])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "075449fc-f693-41d7-9e44-7f3805093b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1404,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68916712-8f1e-4df9-bc91-2696f3f67501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize input and label\n",
    "text = \"[CLS] 今天/吃饭/我/妈妈/家/去 [SEP]\"\n",
    "label_text = \"今天我去妈妈家吃饭\"\n",
    "\n",
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "masked_index = 5  \n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor)\n",
    "    predictions = outputs[0]\n",
    "\n",
    "# Confirm we were able to predict the masked word\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "print(f\"Original Text: {text}\")\n",
    "print(f\"Masked Text: {tokenized_text}\")\n",
    "print(f\"Predicted Token: {predicted_token}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
